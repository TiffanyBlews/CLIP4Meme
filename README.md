# CLIP4Meme: åŸºäºCLIPçš„Memeå›¾ç‰‡æ¨èç³»ç»Ÿ
Inspired by [CLIP4Clip](https://github.com/ArrowLuo/CLIP4Clip), [Cap4Video](https://github.com/whwu95/Cap4Video) and [EFB](https://github.com/colalikerlovely/A-Benchmark-for-Cross-Modal-Emotion-Infused-Internet-Meme-Recommendation)

## é¡¹ç›®ç®€ä»‹

CLIP4Memeæ˜¯ä¸€ä¸ªåŸºäºCLIPï¼ˆContrastive Language-Image Pre-trainingï¼‰çš„å¤šæ¨¡æ€Memeå›¾ç‰‡æ£€ç´¢æ¨èç³»ç»Ÿï¼Œé‡‡ç”¨ä¸¤åˆ†æ”¯è®­ç»ƒç­–ç•¥ï¼ˆQI + QCï¼‰æ¥æå‡æ£€ç´¢æ€§èƒ½ã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿç†è§£å›¾åƒã€æ–‡æœ¬å’Œæƒ…æ„Ÿä¿¡æ¯ï¼Œå®ç°é«˜è´¨é‡çš„Memeå†…å®¹æ£€ç´¢ã€‚

## æ ¸å¿ƒç‰¹æ€§

- ğŸ–¼ï¸ **å¤šæ¨¡æ€ç†è§£**: æ”¯æŒå›¾åƒã€æ–‡æœ¬å’Œæƒ…æ„Ÿä¿¡æ¯çš„è”åˆå»ºæ¨¡
- ğŸ”„ **ä¸¤é˜¶æ®µè®­ç»ƒ**: QIï¼ˆQuery-Imageï¼‰å’ŒQCï¼ˆQuery-Contextï¼‰åˆ†æ”¯çš„æ¸è¿›å¼è®­ç»ƒ
- ğŸ§  **æƒ…æ„Ÿèåˆ**: é›†æˆBERTæƒ…æ„Ÿåˆ†æï¼Œå¢å¼ºå›¾åƒç‰¹å¾è¡¨ç¤º
- ğŸ” **è·¨æ¨¡æ€æ³¨æ„åŠ›**: ä½¿ç”¨Co-attentionæœºåˆ¶å®ç°å›¾åƒå’Œæ–‡æœ¬çš„æ·±åº¦äº¤äº’
- âš¡ **é«˜æ•ˆæ¨ç†**: æ”¯æŒå•åˆ†æ”¯å’Œèåˆæ¨ç†æ¨¡å¼

## é¡¹ç›®æ¶æ„

### æ¨¡å‹æ¶æ„ (CLIP4Meme)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Query Text    â”‚    â”‚   Image Input   â”‚    â”‚  Emotion Text   â”‚
â”‚   (CLIP Text)   â”‚    â”‚   (CLIP Vision) â”‚    â”‚    (BERT)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                      â”‚                      â”‚
          â–¼                      â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Query Features â”‚    â”‚  Image Features â”‚    â”‚ Emotion Featuresâ”‚
â”‚   (Pooled)      â”‚    â”‚   (Fused)       â”‚    â”‚   (Projected)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                      â”‚                      â”‚
          â”‚                      â–¼                      â”‚
          â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
          â”‚              â”‚ Co-Attention    â”‚            â”‚
          â”‚              â”‚   Block         â”‚            â”‚
          â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
          â”‚                        â”‚                    â”‚
          â”‚                        â–¼                    â”‚
          â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
          â”‚              â”‚ Fused Image     â”‚            â”‚
          â”‚              â”‚   Features      â”‚            â”‚
          â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
          â”‚                        â”‚                    â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼        â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚     Similarity Computation      â”‚
              â”‚  (QI: Query vs Fused Image)    â”‚
              â”‚  (QC: Query vs Context)        â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### è®­ç»ƒç­–ç•¥

#### é˜¶æ®µä¸€ (Stage 1): QIåˆ†æ”¯è®­ç»ƒ
- **ç›®æ ‡**: å­¦ä¹ æŸ¥è¯¢æ–‡æœ¬ä¸å›¾åƒç‰¹å¾çš„å¯¹åº”å…³ç³»
- **è¾“å…¥**: Query + Image + Emotion + Context
- **è¾“å‡º**: Query-Imageç›¸ä¼¼åº¦çŸ©é˜µ
- **ç‰¹ç‚¹**: ä½¿ç”¨Co-attentionèåˆå›¾åƒå’Œä¸Šä¸‹æ–‡ä¿¡æ¯

#### é˜¶æ®µäºŒ (Stage 2): QCåˆ†æ”¯è®­ç»ƒ
- **ç›®æ ‡**: å­¦ä¹ æŸ¥è¯¢æ–‡æœ¬ä¸å€™é€‰æ–‡æœ¬çš„å¯¹åº”å…³ç³»
- **è¾“å…¥**: Query + Context
- **è¾“å‡º**: Query-Contextç›¸ä¼¼åº¦çŸ©é˜µ
- **ç‰¹ç‚¹**: çº¯æ–‡æœ¬å¯¹æ¯”å­¦ä¹ ï¼Œä¸æ¶‰åŠå›¾åƒ


## å®‰è£…æŒ‡å—

1. **å…‹éš†é¡¹ç›®**
```bash
git clone https://github.com/TiffanyBlews/CLIP4Meme
cd CLIP4Meme
```

2. **å®‰è£…ä¾èµ–**
```bash
pip install -r requirements.txt
```

3. **ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹**
```bash
# CLIPæ¨¡å‹ä¼šè‡ªåŠ¨ä¸‹è½½
# BERTä¸­æ–‡æ¨¡å‹ä¼šè‡ªåŠ¨ä¸‹è½½
```

## æ•°æ®å‡†å¤‡

### æ•°æ®æ ¼å¼

é¡¹ç›®æ”¯æŒMSRVTTæ ¼å¼çš„æ•°æ®é›†ï¼Œéœ€è¦ä»¥ä¸‹æ–‡ä»¶ï¼š

```
data/
â”œâ”€â”€ input_file/
â”‚   â”œâ”€â”€ train_video_id_9k.csv      # è®­ç»ƒé›†è§†é¢‘IDåˆ—è¡¨
â”‚   â”œâ”€â”€ train_9k.json              # è®­ç»ƒé›†æ ‡æ³¨æ–‡ä»¶
â”‚   â”œâ”€â”€ train_emotion.json         # è®­ç»ƒé›†æƒ…æ„Ÿæ ‡æ³¨
â”‚   â”œâ”€â”€ test_4k.json               # æµ‹è¯•é›†æ ‡æ³¨æ–‡ä»¶
â”‚   â””â”€â”€ test_emotion.json          # æµ‹è¯•é›†æƒ…æ„Ÿæ ‡æ³¨
â””â”€â”€ image/
    â”œâ”€â”€ video_id_1.jpg             # è§†é¢‘å¸§å›¾åƒ
    â”œâ”€â”€ video_id_2.jpg
    â””â”€â”€ ...
```

### æ•°æ®æ–‡ä»¶ç»“æ„

#### CSVæ–‡ä»¶æ ¼å¼
```csv
video_id
video_001
video_002
...
```

#### JSONæ–‡ä»¶æ ¼å¼
```json
{
  "sentences": [
    {
      "video_id": "video_001",
      "caption": "Ground Truth"
    }
  ],
  "titles": {
    "video_001": ["VLM ç”Ÿæˆçš„å›¾ç‰‡å†…å®¹æè¿°"]
  }
}
```

#### æƒ…æ„Ÿæ ‡æ³¨æ ¼å¼
```json
[
  {
    "video_id": "video_001",
    "emotion": [
      {"person_1": "å¼€å¿ƒ"},
      {"person_2": "å¹½é»˜"}
    ]
  }
]
```

## ä½¿ç”¨æ–¹æ³•

### 1. è®­ç»ƒæ¨¡å‹

#### é˜¶æ®µä¸€è®­ç»ƒ (QIåˆ†æ”¯)
```bash
python train.py --stage 1 \
    --data_path /path/to/data \
    --image_path /path/to/images \
    --batch_size 64 \
    --epochs 20 \
    --lr 5e-4 \
    --save_dir ./checkpoints
```

#### é˜¶æ®µäºŒè®­ç»ƒ (QCåˆ†æ”¯)
```bash
python train.py --stage 2 \
    --data_path /path/to/data \
    --image_path /path/to/images \
    --batch_size 64 \
    --epochs 20 \
    --lr 5e-4 \
    --save_dir ./checkpoints \
    --stage1_checkpoint_path ./checkpoints/stage1_qi_best.pth
```

### 2. æ¨ç†è¯„ä¼°

#### èåˆæ¨ç†
```bash
python inference_fusion.py \
    --checkpoint ./checkpoints/stage2_qc_best.pth \
    --data_path /path/to/data \
    --image_path /path/to/images \
    --alpha 0.6 \
    --compare
```

#### å‚æ•°è¯´æ˜
- `--alpha`: QIåˆ†æ”¯æƒé‡ï¼ŒQCåˆ†æ”¯æƒé‡ä¸º(1-alpha)
- `--compare`: æ˜¯å¦æ¯”è¾ƒæ‰€æœ‰åˆ†æ”¯çš„æ€§èƒ½
- `--temperature`: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶ç›¸ä¼¼åº¦åˆ†å¸ƒ

### 3. æ¨¡å‹é…ç½®

åœ¨`train.py`ä¸­å¯ä»¥è°ƒæ•´ä»¥ä¸‹é…ç½®ï¼š

```python
CONFIG = {
    "batch_size": 64,           # æ‰¹æ¬¡å¤§å°
    "epochs": 20,               # è®­ç»ƒè½®æ•°
    "lr": 5e-4,                # åŸºç¡€å­¦ä¹ ç‡
    "coef_lr": 1e-3,           # CLIPå­¦ä¹ ç‡ç³»æ•°
    "weight_decay": 0.01,      # æƒé‡è¡°å‡
    "pretrained_clip_name": "ViT-B/32",  # CLIPæ¨¡å‹ç‰ˆæœ¬
    "use_emotion_fusion": True, # æ˜¯å¦ä½¿ç”¨æƒ…æ„Ÿèåˆ
}
```

## æ ¸å¿ƒæ¨¡å—è¯´æ˜

### 1. æ¨¡å‹æ¨¡å— (`model.py`)

- **CLIP4Meme**: ä¸»æ¨¡å‹ç±»ï¼ŒåŒ…å«QIå’ŒQCä¸¤ä¸ªåˆ†æ”¯
- **ç‰¹å¾ç¼–ç **: æ”¯æŒå›¾åƒã€æ–‡æœ¬å’Œæƒ…æ„Ÿçš„ç¼–ç 
- **è·¨æ¨¡æ€èåˆ**: ä½¿ç”¨Co-attentionå®ç°å›¾åƒ-æ–‡æœ¬äº¤äº’
- **æ¨ç†èåˆ**: æ”¯æŒQIå’ŒQCåˆ†æ”¯çš„åŠ æƒèåˆ

### 2. æ•°æ®é›†æ¨¡å— (`dataset.py`)

- **MSRVTT_Dataset**: è‡ªå®šä¹‰æ•°æ®é›†ç±»
- **åŠ¨æ€å›¾åƒåŠ è½½**: æ”¯æŒæŒ‰éœ€åŠ è½½å›¾åƒï¼ˆé˜¶æ®µäºŒè®­ç»ƒæ—¶è·³è¿‡ï¼‰
- **å¤šæ¨¡æ€å¤„ç†**: ç»Ÿä¸€å¤„ç†æ–‡æœ¬ã€å›¾åƒå’Œæƒ…æ„Ÿæ•°æ®

### 3. è®­ç»ƒæ¨¡å— (`train.py`)

- **ä¸¤é˜¶æ®µè®­ç»ƒ**: æ”¯æŒQIå’ŒQCåˆ†æ”¯çš„ç‹¬ç«‹è®­ç»ƒ
- **ä¼˜åŒ–å™¨é…ç½®**: é’ˆå¯¹ä¸åŒæ¨¡å—è®¾ç½®ä¸åŒçš„å­¦ä¹ ç‡
- **è¯„ä¼°é€»è¾‘**: å®Œæ•´çš„R@kè¯„ä¼°å’Œæ¨¡å‹ä¿å­˜

### 4. æ¨ç†æ¨¡å— (`inference_fusion.py`)

- **èåˆæ¨ç†**: æ”¯æŒQI+QCçš„åŠ æƒèåˆ
- **æ€§èƒ½æ¯”è¾ƒ**: æ¯”è¾ƒä¸åŒåˆ†æ”¯å’Œèåˆæ–¹æ³•çš„æ€§èƒ½
- **è¯¦ç»†åˆ†æ**: æä¾›é¢„æµ‹ç»“æœçš„è¯¦ç»†åˆ†æ

## æ€§èƒ½æŒ‡æ ‡

ç³»ç»Ÿä½¿ç”¨ä»¥ä¸‹æŒ‡æ ‡è¯„ä¼°æ€§èƒ½ï¼š

- **R@1**: æ’åç¬¬ä¸€çš„æ­£ç¡®ç‡
- **R@5**: æ’åå‰äº”çš„æ­£ç¡®ç‡  
- **R@10**: æ’åå‰åçš„æ­£ç¡®ç‡


## è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ï¼Œè¯¦è§LICENSEæ–‡ä»¶ã€‚

## å¼•ç”¨

å¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†æœ¬é¡¹ç›®ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex

```
